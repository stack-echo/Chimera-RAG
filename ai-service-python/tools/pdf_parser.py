import logging
import io
from pathlib import Path

# Docling æ ¸å¿ƒ
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat, DocumentStream
from docling.datamodel.pipeline_options import PdfPipelineOptions, TableStructureOptions

# ğŸ”¥ æ–°å¢ï¼šDocling åŸç”Ÿåˆ‡åˆ†å™¨
from docling.chunking import HybridChunker

class PDFParser:
    _converter = None
    _chunker = None

    @classmethod
    def _get_components(cls):
        """å•ä¾‹æ¨¡å¼åˆå§‹åŒ– Converter å’Œ Chunker"""
        if cls._converter is None:
            logging.info("ğŸ¢ [Init] æ­£åœ¨åˆå§‹åŒ– Docling æ¨¡å‹...")

            # 1. é…ç½®è½¬æ¢å™¨
            pipeline_options = PdfPipelineOptions()
            pipeline_options.do_ocr = False
            pipeline_options.do_table_structure = True

            cls._converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            )

            # 2. é…ç½®åˆ‡åˆ†å™¨ (HybridChunker)
            # å®ƒå¯ä»¥æ™ºèƒ½åœ°ç»“åˆâ€œè¯­ä¹‰ç»“æ„â€å’Œâ€œTokené™åˆ¶â€æ¥åˆ‡åˆ†
            cls._chunker = HybridChunker(
                tokenizer="sentence-transformers/all-MiniLM-L6-v2", # ç”¨å’Œ Embedding ä¸€æ ·çš„ tokenizer ä¼°ç®—é•¿åº¦
                max_tokens=500, # æ¯ä¸ªå—çš„æœ€å¤§ Token æ•°
                merge_peers=True, # åˆå¹¶åŒçº§æ ‡é¢˜ä¸‹çš„å†…å®¹
            )

            logging.info("âœ… [Init] Docling ç»„ä»¶å°±ç»ª")
        return cls._converter, cls._chunker

    @staticmethod
    def parse_and_chunk(file_source, filename="temp.pdf"):
        """
        è§£æ PDF å¹¶è¿”å›å¸¦æœ‰ã€çœŸå®é¡µç ã€‘çš„è¯­ä¹‰åˆ‡ç‰‡
        """
        converter, chunker = PDFParser._get_components()
        logging.info(f"ğŸ“„ [Docling] å¼€å§‹è§£æ: {filename}")

        try:
            # 1. æ„å»ºè¾“å…¥æº
            input_doc = None
            if isinstance(file_source, bytes):
                input_doc = DocumentStream(name=filename, stream=io.BytesIO(file_source))
            else:
                input_doc = Path(file_source)

            # 2. æ‰§è¡Œè½¬æ¢ (PDF -> DL Document)
            # è¿™ä¸€æ­¥æ¯”è¾ƒè€—æ—¶ (CPU/MPS)
            conv_result = converter.convert(input_doc)
            doc = conv_result.document
            logging.info(f"âœ… [Docling] è½¬æ¢å®Œæˆï¼Œå¼€å§‹æå–åˆ‡ç‰‡...")

            # 3. ä½¿ç”¨ HybridChunker åˆ‡åˆ† (æå–çœŸå®é¡µç çš„æ ¸å¿ƒæ­¥éª¤)
            # chunker.chunk(doc) è¿”å›çš„æ˜¯ Docling çš„ Chunk å¯¹è±¡è¿­ä»£å™¨
            chunk_iter = chunker.chunk(doc)

            final_chunks = []
            for i, chunk in enumerate(chunk_iter):
                # chunk.text: åŒ…å«äº†æ ‡é¢˜ä¸Šä¸‹æ–‡çš„æ–‡æœ¬ (ä¾‹å¦‚: "Header1 > Header2 \n æ­£æ–‡...")
                # chunk.meta: åŒ…å«äº†å…ƒæ•°æ®

                # ğŸ”¥ æå–é¡µç 
                # Docling çš„ chunk å¯èƒ½è·¨é¡µï¼Œæˆ‘ä»¬å–è¿™ä¸ª chunk å‡ºç°çš„â€œç¬¬ä¸€é¡µâ€ä½œä¸ºè·³è½¬ç›®æ ‡
                page_num = 1
                if chunk.meta.doc_items:
                    # è¿½æº¯è¿™ä¸ª chunk æ¥æºäºæ–‡æ¡£çš„å“ªä¸ªéƒ¨åˆ† (Provenance)
                    first_item = chunk.meta.doc_items[0]
                    if hasattr(first_item, 'prov') and first_item.prov:
                        page_num = first_item.prov[0].page_no

                # åºåˆ—åŒ–ç»“æœ
                final_chunks.append({
                    "content": chunk.text, # HybridChunker è‡ªåŠ¨å¸®ä½ æ‹¼å¥½äº†ä¸Šä¸‹æ–‡ï¼Œä¸éœ€è¦æ‰‹åŠ¨ join æ ‡é¢˜äº†
                    "page": page_num       # âœ… çœŸå®çš„é¡µç ï¼
                })

            logging.info(f"âœ‚ï¸ [HybridChunker] ç”Ÿæˆäº† {len(final_chunks)} ä¸ªå¸¦æœ‰é¡µç çš„ç‰‡æ®µ")

            # æ‰“å°å‰3ä¸ªçœ‹çœ‹æ•ˆæœ
            for idx, c in enumerate(final_chunks[:3]):
                logging.info(f"   ğŸ”¹ P{c['page']}: {c['content'][:50]}...")

            return final_chunks

        except Exception as e:
            logging.error(f"âŒ [Docling] è§£æå¤±è´¥: {e}", exc_info=True)
            return []