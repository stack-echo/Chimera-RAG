import sys
import os
import time
import logging
from dotenv import load_dotenv
from concurrent import futures
import grpc
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import fitz

sys.path.append(os.path.join(os.path.dirname(__file__), 'rpc'))

# å¯¼å…¥ç”Ÿæˆçš„ä»£ç 
import rag_service_pb2
import rag_service_pb2_grpc

# 1. åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# 2. ä»ç¯å¢ƒå˜é‡è¯»å– (å¦‚æœè¯»ä¸åˆ°ï¼Œå¯ä»¥ç»™ä¸ªé»˜è®¤å€¼æˆ–è€…æŠ¥é”™)
API_KEY = os.getenv("DEEPSEEK_API_KEY")
BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

if not API_KEY:
    raise ValueError("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ï¼")

# --- åˆå§‹åŒ– ---
print("ğŸ“¥ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
try:
    # å°è¯•ä»æœ¬åœ°åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä¸‹è½½
    model_dir = snapshot_download('AI-ModelScope/all-MiniLM-L6-v2')
    embed_model = SentenceTransformer(model_dir)
except:
    embed_model = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… Embedding æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")

# åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
llm_client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# --- ä¸šåŠ¡é€»è¾‘å®ç° ---
class ChimeraLLMService(rag_service_pb2_grpc.LLMServiceServicer):

    def AskStream(self, request, context):
            """
            æ ¸å¿ƒé—®ç­”æ¥å£ï¼šæ¥æ”¶ Prompt -> è°ƒç”¨ LLM -> æµå¼è¿”å›
            """
            query = request.query # è¿™é‡Œçš„ query å®é™…ä¸Šæ˜¯ Go æ‹¼è£…å¥½çš„ Prompt (åŒ…å«ä¸Šä¸‹æ–‡)
            print(f"[LLM] æ”¶åˆ° Promptï¼Œå‡†å¤‡ç”Ÿæˆå›ç­”...")

            # 1. è°ƒç”¨ DeepSeek API
            try:
                response = llm_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„EHSå®‰å…¨åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚"},
                        {"role": "user", "content": query},
                    ],
                    stream=True # å¼€å¯æµå¼
                )

                # 2. æµå¼è½¬å‘ç»™ Go
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        yield rag_service_pb2.AskResponse(answer_delta=content)

            except Exception as e:
                print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                yield rag_service_pb2.AskResponse(answer_delta=f"[Error] å¤§æ¨¡å‹æœåŠ¡å¼‚å¸¸: {str(e)}")

    def EmbedData(self, request, context):
            """
            ã€çœŸå®ã€‘å‘é‡åŒ–æ¥å£
            """
            start = time.time()

            # 1. æå–æ–‡æœ¬
            text = ""
            if request.text:
                text = request.text
            elif request.image_url:
                text = "Image embedding not implemented yet" # æš‚æ—¶è·³è¿‡å›¾ç‰‡

            print(f"[å‘é‡åŒ–è¯·æ±‚] æ­£åœ¨å¤„ç†æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")

            # 2. è°ƒç”¨æ¨¡å‹æ¨ç† (Inference)
            # tolist() æ˜¯ä¸ºäº†æŠŠ numpy æ•°ç»„è½¬ä¸º Python listï¼Œå¦åˆ™ gRPC ä¼ ä¸è¿‡å»
            vector = embed_model.encode(text).tolist()

            duration = (time.time() - start) * 1000
            print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶: {duration:.2f}msï¼Œç»´åº¦: {len(vector)}")

            # 3. è¿”å›çœŸå®å‘é‡
            return rag_service_pb2.EmbedResponse(
                vector=vector
            )

    def ParseAndEmbed(self, request, context):
            """
            æ ¸å¿ƒåŠŸèƒ½ï¼šPDF è§£æ -> æ–‡æœ¬æ¸…æ´— -> æ™ºèƒ½åˆ‡ç‰‡ -> æ‰¹é‡å‘é‡åŒ–
            """
            file_name = request.file_name
            print(f"ğŸ“„ [Parse] å¼€å§‹å¤„ç†æ–‡ä»¶: {file_name}, å¤§å°: {len(request.file_content)} bytes")

            # 1. æ‰“å¼€ PDF (ä»å†…å­˜å­—èŠ‚æµ)
            try:
                doc = fitz.open(stream=request.file_content, filetype="pdf")
            except Exception as e:
                print(f"âŒ PDF è§£æå¤±è´¥: {e}")
                return rag_service_pb2.ParseResponse()

            # 2. æå–æ–‡æœ¬ (æŒ‰é¡µ)
            full_chunks = []

            for page_num, page in enumerate(doc):
                text = page.get_text()
                if not text.strip():
                    continue

                # --- ç®€å•çš„åˆ‡ç‰‡é€»è¾‘ (Chunking) ---
                # çœŸå®åœºæ™¯å¯ä»¥ç”¨ LangChain çš„ RecursiveCharacterTextSplitter
                # è¿™é‡Œæˆ‘ä»¬æ‰‹å†™ä¸€ä¸ªç®€å•çš„ï¼šæ¯ 300 å­—ç¬¦åˆ‡ä¸€æ®µï¼Œé‡å  50 å­—ç¬¦
                chunk_size = 300
                overlap = 50

                start = 0
                while start < len(text):
                    end = start + chunk_size
                    segment = text[start:end]

                    # 3. å‘é‡åŒ–è¯¥ç‰‡æ®µ
                    # æ³¨æ„ï¼šè¿™é‡Œæ˜¯ä¸²è¡Œè°ƒç”¨ï¼Œå¤§é‡æ•°æ®å¯ä»¥æ”¹ä¸ºæ‰¹é‡ encode
                    vector = embed_model.encode(segment).tolist()

                    chunk_obj = rag_service_pb2.DocChunk(
                        content=segment,
                        vector=vector,
                        page_number=page_num + 1 # äººç±»é˜…è¯»ä¹ æƒ¯ä»1å¼€å§‹
                    )
                    full_chunks.append(chunk_obj)

                    # æ»‘åŠ¨çª—å£
                    start += (chunk_size - overlap)

            print(f"âœ… [Parse] å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(full_chunks)} ä¸ªåˆ‡ç‰‡")
            return rag_service_pb2.ParseResponse(chunks=full_chunks)

# --- æœåŠ¡å™¨å¯åŠ¨é€»è¾‘ ---
def serve():
    # åˆ›å»º gRPC æœåŠ¡å™¨
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))

    # æ³¨å†Œæˆ‘ä»¬çš„æœåŠ¡
    rag_service_pb2_grpc.add_LLMServiceServicer_to_server(ChimeraLLMService(), server)

    # ç›‘å¬ç«¯å£
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ Chimera Brain is running on port 50051...")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    logging.basicConfig()
    serve()