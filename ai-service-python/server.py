import sys
import os
import time
import logging
from concurrent import futures
import grpc
from sentence_transformers import SentenceTransformer

sys.path.append(os.path.join(os.path.dirname(__file__), 'rpc'))

# å¯¼å…¥ç”Ÿæˆçš„ä»£ç 
import rag_service_pb2
import rag_service_pb2_grpc

# --- åˆå§‹åŒ– AI æ¨¡å‹ ---
print("ğŸ“¥ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (all-MiniLM-L6-v2)...")
# è¿™ä¸ªæ¨¡å‹å¾ˆå°(80MB)ï¼Œä¸‹è½½å¾ˆå¿«ï¼Œç”Ÿæˆ 384 ç»´å‘é‡
embed_model = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")

# --- ä¸šåŠ¡é€»è¾‘å®ç° ---
class ChimeraLLMService(rag_service_pb2_grpc.LLMServiceServicer):

    def AskStream(self, request, context):
            """
            æš‚æ—¶è¿˜ä¿ç•™ Mock å¯¹è¯ï¼Œä¸‹ä¸€æ­¥å†æ¥å…¥ DeepSeek/OpenAI
            """
            query = request.query
            print(f"[æ”¶åˆ°æé—®] {query}")

            yield rag_service_pb2.AskResponse(thinking_log=f"æ­£åœ¨è®¡ç®—æŸ¥è¯¢å‘é‡ (384ç»´)...")
            
            # è¿™é‡Œæ¼”ç¤ºä¸€ä¸‹ï¼šæˆ‘ä»¬çœŸçš„å»ç®—ä¸€ä¸‹æé—®çš„å‘é‡
            q_vector = embed_model.encode(query).tolist()
            yield rag_service_pb2.AskResponse(thinking_log=f"å‘é‡è®¡ç®—å®Œæ¯•ï¼Œç»´åº¦: {len(q_vector)}")
            time.sleep(0.5)

            yield rag_service_pb2.AskResponse(answer_delta="è¿™æ˜¯ Python ç«¯é›†æˆ HuggingFace æ¨¡å‹åçš„æµ‹è¯•å›å¤ã€‚")

    def EmbedData(self, request, context):
            """
            ã€çœŸå®ã€‘å‘é‡åŒ–æ¥å£
            """
            start = time.time()

            # 1. æå–æ–‡æœ¬
            text = ""
            if request.text:
                text = request.text
            elif request.image_url:
                text = "Image embedding not implemented yet" # æš‚æ—¶è·³è¿‡å›¾ç‰‡

            print(f"[å‘é‡åŒ–è¯·æ±‚] æ­£åœ¨å¤„ç†æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")

            # 2. è°ƒç”¨æ¨¡å‹æ¨ç† (Inference)
            # tolist() æ˜¯ä¸ºäº†æŠŠ numpy æ•°ç»„è½¬ä¸º Python listï¼Œå¦åˆ™ gRPC ä¼ ä¸è¿‡å»
            vector = embed_model.encode(text).tolist()

            duration = (time.time() - start) * 1000
            print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶: {duration:.2f}msï¼Œç»´åº¦: {len(vector)}")

            # 3. è¿”å›çœŸå®å‘é‡
            return rag_service_pb2.EmbedResponse(
                vector=vector
            )

# --- æœåŠ¡å™¨å¯åŠ¨é€»è¾‘ ---
def serve():
    # åˆ›å»º gRPC æœåŠ¡å™¨
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))

    # æ³¨å†Œæˆ‘ä»¬çš„æœåŠ¡
    rag_service_pb2_grpc.add_LLMServiceServicer_to_server(ChimeraLLMService(), server)

    # ç›‘å¬ç«¯å£
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ Chimera Brain is running on port 50051...")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    logging.basicConfig()
    serve()