import sys
import os
import time
import logging
from dotenv import load_dotenv
from concurrent import futures
import grpc
from sentence_transformers import SentenceTransformer
from openai import OpenAI

sys.path.append(os.path.join(os.path.dirname(__file__), 'rpc'))

# å¯¼å…¥ç”Ÿæˆçš„ä»£ç 
import rag_service_pb2
import rag_service_pb2_grpc

# 1. åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# 2. ä»ç¯å¢ƒå˜é‡è¯»å– (å¦‚æœè¯»ä¸åˆ°ï¼Œå¯ä»¥ç»™ä¸ªé»˜è®¤å€¼æˆ–è€…æŠ¥é”™)
API_KEY = os.getenv("DEEPSEEK_API_KEY")
BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

if not API_KEY:
    raise ValueError("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ï¼")

# --- åˆå§‹åŒ– ---
print("ğŸ“¥ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
try:
    # å°è¯•ä»æœ¬åœ°åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä¸‹è½½
    model_dir = snapshot_download('AI-ModelScope/all-MiniLM-L6-v2')
    embed_model = SentenceTransformer(model_dir)
except:
    embed_model = SentenceTransformer('all-MiniLM-L6-v2')
print("âœ… Embedding æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")

# åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
llm_client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# --- ä¸šåŠ¡é€»è¾‘å®ç° ---
class ChimeraLLMService(rag_service_pb2_grpc.LLMServiceServicer):

    def AskStream(self, request, context):
            """
            æ ¸å¿ƒé—®ç­”æ¥å£ï¼šæ¥æ”¶ Prompt -> è°ƒç”¨ LLM -> æµå¼è¿”å›
            """
            query = request.query # è¿™é‡Œçš„ query å®é™…ä¸Šæ˜¯ Go æ‹¼è£…å¥½çš„ Prompt (åŒ…å«ä¸Šä¸‹æ–‡)
            print(f"[LLM] æ”¶åˆ° Promptï¼Œå‡†å¤‡ç”Ÿæˆå›ç­”...")

            # 1. è°ƒç”¨ DeepSeek API
            try:
                response = llm_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„EHSå®‰å…¨åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚"},
                        {"role": "user", "content": query},
                    ],
                    stream=True # å¼€å¯æµå¼
                )

                # 2. æµå¼è½¬å‘ç»™ Go
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        yield rag_service_pb2.AskResponse(answer_delta=content)

            except Exception as e:
                print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                yield rag_service_pb2.AskResponse(answer_delta=f"[Error] å¤§æ¨¡å‹æœåŠ¡å¼‚å¸¸: {str(e)}")

    def EmbedData(self, request, context):
            """
            ã€çœŸå®ã€‘å‘é‡åŒ–æ¥å£
            """
            start = time.time()

            # 1. æå–æ–‡æœ¬
            text = ""
            if request.text:
                text = request.text
            elif request.image_url:
                text = "Image embedding not implemented yet" # æš‚æ—¶è·³è¿‡å›¾ç‰‡

            print(f"[å‘é‡åŒ–è¯·æ±‚] æ­£åœ¨å¤„ç†æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)}")

            # 2. è°ƒç”¨æ¨¡å‹æ¨ç† (Inference)
            # tolist() æ˜¯ä¸ºäº†æŠŠ numpy æ•°ç»„è½¬ä¸º Python listï¼Œå¦åˆ™ gRPC ä¼ ä¸è¿‡å»
            vector = embed_model.encode(text).tolist()

            duration = (time.time() - start) * 1000
            print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶: {duration:.2f}msï¼Œç»´åº¦: {len(vector)}")

            # 3. è¿”å›çœŸå®å‘é‡
            return rag_service_pb2.EmbedResponse(
                vector=vector
            )

# --- æœåŠ¡å™¨å¯åŠ¨é€»è¾‘ ---
def serve():
    # åˆ›å»º gRPC æœåŠ¡å™¨
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))

    # æ³¨å†Œæˆ‘ä»¬çš„æœåŠ¡
    rag_service_pb2_grpc.add_LLMServiceServicer_to_server(ChimeraLLMService(), server)

    # ç›‘å¬ç«¯å£
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ Chimera Brain is running on port 50051...")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    logging.basicConfig()
    serve()