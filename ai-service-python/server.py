import sys
import os
import time
import logging
from concurrent import futures
import grpc

sys.path.append(os.path.join(os.path.dirname(__file__), 'rpc'))

# å¯¼å…¥ç”Ÿæˆçš„ä»£ç 
import rag_service_pb2
import rag_service_pb2_grpc

# --- ä¸šåŠ¡é€»è¾‘å®ç° ---
class ChimeraLLMService(rag_service_pb2_grpc.LLMServiceServicer):

    def AskStream(self, request, context):
        """
        æ¨¡æ‹Ÿæµå¼é—®ç­”æ¥å£
        """
        query = request.query
        print(f"[æ”¶åˆ°è¯·æ±‚] Query: {query} | UseGraph: {request.use_graph}")

        # 1. æ¨¡æ‹Ÿ "æ€è€ƒè¿‡ç¨‹" (Thinking Log)
        yield rag_service_pb2.AskResponse(
            thinking_log=f"æ­£åœ¨åˆ†ææ„å›¾... (Mock ID: {request.session_id})"
        )
        time.sleep(0.5) # å‡è£…åœ¨æ€è€ƒ

        if request.use_graph:
            yield rag_service_pb2.AskResponse(
                thinking_log="æ£€æµ‹åˆ°ä¸“ä¸šæœ¯è¯­ï¼Œæ­£åœ¨æŸ¥è¯¢ NebulaGraph çŸ¥è¯†å›¾è°±..."
            )
            time.sleep(0.5)

        # 2. æ¨¡æ‹Ÿ "æµå¼åå­—" (Answer Delta)
        # å‡è£…è¿™æ˜¯ LLM ç”Ÿæˆçš„å›å¤
        mock_answer = f"è¿™æ˜¯ Chimera é’ˆå¯¹é—®é¢˜ '{query}' çš„æ¨¡æ‹Ÿå›ç­”ã€‚"
        for char in mock_answer:
            yield rag_service_pb2.AskResponse(
                answer_delta=char
            )
            time.sleep(0.1) # æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ

        # 3. æ¨¡æ‹Ÿ "å¼•ç”¨æ¥æº" (Source Docs)
        # æœ€åä¸€æ¬¡è¿”å›å¸¦ä¸Šå¼•ç”¨
        final_resp = rag_service_pb2.AskResponse()
        doc1 = final_resp.source_docs.add()
        doc1.doc_name = "å±åŒ–å“å®‰å…¨æ‰‹å†Œ_v1.pdf"
        doc1.page_num = "12"
        doc1.score = 0.95
        yield final_resp

    def EmbedData(self, request, context):
        """
        æ¨¡æ‹Ÿå‘é‡åŒ–æ¥å£
        """
        print(f"[å‘é‡åŒ–è¯·æ±‚] Type: {'Image' if request.image_url else 'Text'}")

        # æ¨¡æ‹Ÿè¿”å›ä¸€ä¸ª 4 ç»´å‘é‡ (çœŸå®åœºæ™¯æ˜¯ 768 æˆ– 1024 ç»´)
        return rag_service_pb2.EmbedResponse(
            vector=[0.1, 0.2, 0.3, 0.99]
        )

# --- æœåŠ¡å™¨å¯åŠ¨é€»è¾‘ ---
def serve():
    # åˆ›å»º gRPC æœåŠ¡å™¨
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))

    # æ³¨å†Œæˆ‘ä»¬çš„æœåŠ¡
    rag_service_pb2_grpc.add_LLMServiceServicer_to_server(ChimeraLLMService(), server)

    # ç›‘å¬ç«¯å£
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ Chimera Brain is running on port 50051...")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    logging.basicConfig()
    serve()