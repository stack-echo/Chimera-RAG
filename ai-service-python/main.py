import grpc
from concurrent import futures
import logging
import os
import tempfile
from pathlib import Path

# å¼•å…¥ç”Ÿæˆçš„ gRPC ä»£ç 
import rag_pb2
import rag_pb2_grpc

# ğŸ”¥ Docling æ ¸å¿ƒç»„ä»¶
from docling.document_converter import DocumentConverter
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions

# ğŸ”¥ LangChain åˆ‡åˆ†å·¥å…· (æŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†)
from langchain_text_splitters import MarkdownHeaderTextSplitter

class LLMService(rag_pb2_grpc.LLMServiceServicer):
    def __init__(self):
        logging.info("æ­£åœ¨åˆå§‹åŒ– Docling Converter...")
        # é…ç½® Docling
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = False  # å¦‚æœæ˜¯çº¯æ–‡æœ¬PDFï¼Œå…³æ‰OCRé€Ÿåº¦å¿«ï¼›æ‰«æä»¶è¯·å¼€å¯
        pipeline_options.do_table_structure = True # å¼€å¯è¡¨æ ¼è§£æ

        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: pipeline_options
            }
        )
        logging.info("âœ… Docling åˆå§‹åŒ–å®Œæˆ")

    def ParsePDF(self, request, context):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šæ¥æ”¶ PDF URL/Path -> ä¸‹è½½/è¯»å– -> Docling è½¬ Markdown -> æ™ºèƒ½åˆ‡åˆ† -> è¿”å› Chunk
        """
        file_path = request.file_path
        logging.info(f"æ”¶åˆ°è§£æä»»åŠ¡: {file_path}")

        # 1. ä¸´æ—¶ä¿å­˜/è¯»å–æ–‡ä»¶
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ backend-go ä¼ è¿‡æ¥çš„æ˜¯æœ¬åœ°è·¯å¾„ (minio æŒ‚è½½æˆ–è€…æ˜¯ä¸‹è½½åçš„è·¯å¾„)
        # å¦‚æœæ˜¯ URLï¼ŒDocling ä¹Ÿæ”¯æŒç›´æ¥ä¼  URL

        if not os.path.exists(file_path):
             # ç®€å•çš„å®¹é”™ï¼Œé˜²æ­¢è·¯å¾„ä¸å¯¹
             context.set_code(grpc.StatusCode.NOT_FOUND)
             context.set_details(f"File not found: {file_path}")
             return rag_pb2.ParseResponse()

        try:
            # 2. ğŸ”¥ Docling æ ¸å¿ƒè§£æï¼šPDF -> Markdown
            logging.info("å¼€å§‹ Docling è§£æ (å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
            conv_result = self.converter.convert(file_path)

            # è·å– Markdown å†…å®¹
            markdown_content = conv_result.document.export_to_markdown()
            logging.info(f"è§£æå®Œæˆï¼ŒMarkdown é•¿åº¦: {len(markdown_content)}")

            # 3. ğŸ”¥ æ™ºèƒ½åˆ‡åˆ† (Semantic Chunking)
            # å®šä¹‰æƒ³è¦ä½œä¸ºåˆ‡åˆ†ç‚¹çš„ Header çº§åˆ«
            headers_to_split_on = [
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]

            markdown_splitter = MarkdownHeaderTextSplitter(
                headers_to_split_on=headers_to_split_on,
                strip_headers=False # ä¿ç•™æ ‡é¢˜åœ¨å†…å®¹é‡Œï¼Œè®©ä¸Šä¸‹æ–‡æ›´æ¸…æ™°
            )

            docs = markdown_splitter.split_text(markdown_content)

            logging.info(f"æ™ºèƒ½åˆ‡åˆ†å®Œæˆï¼Œå…± {len(docs)} ä¸ªç‰‡æ®µ")

            # 4. ç»„è£…è¿”å›ç»“æœ
            chunks = []
            for i, doc in enumerate(docs):
                # ç»„åˆå…ƒæ•°æ®å’Œå†…å®¹
                # doc.page_content æ˜¯çº¯æ–‡æœ¬
                # doc.metadata åŒ…å« {'Header 1': '...', 'Header 2': '...'}

                # æˆ‘ä»¬å¯ä»¥æŠŠæ ‡é¢˜æ‹¼å›åˆ°å†…å®¹å‰é¢ï¼Œå¢å¼ºè¯­ä¹‰
                header_context = " > ".join(doc.metadata.values())
                final_content = f"ã€ç« èŠ‚: {header_context}ã€‘\n{doc.page_content}"

                chunks.append(rag_pb2.Chunk(
                    content=final_content,
                    page_number=1 # Docling ç›®å‰è½¬ Markdown åé¡µç å¯¹é½æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶ç”± Go ç«¯å¤„ç†æˆ–å¡« 1
                ))

            return rag_pb2.ParseResponse(chunks=chunks)

        except Exception as e:
            logging.error(f"è§£æå¤±è´¥: {str(e)}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return rag_pb2.ParseResponse()

    def Embed(self, request, context):
        # ... (Embed ä»£ç ä¿æŒä¸å˜ï¼Œæˆ–è€…æš‚æ—¶ç•™ç©ºï¼Œå¦‚æœä½ è¿˜åœ¨ç”¨æ¨¡æ‹Ÿ Embed) ...
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œå…ˆè¿”å›æ¨¡æ‹Ÿå‘é‡
        return rag_pb2.EmbedResponse(
            vectors=[rag_pb2.Vector(data=[0.1] * 768) for _ in request.documents]
        )

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    rag_pb2_grpc.add_LLMServiceServicer_to_server(LLMService(), server)
    server.add_insecure_port('[::]:50051')
    logging.info("ğŸš€ Python AI Service (Doclingç‰ˆ) å·²å¯åŠ¨ï¼Œç›‘å¬ 50051...")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    serve()